Training Feed-Forward Networks
=================================
This tutorial serves as a primer for using the Shark implementation of feed-forward
multi-layer perceptron neural networks [Bishop1995]_. The whole functionality can be discovered
in the doxygen pages of the :doxy:`FFNet` class. It is recommended to read the
getting started section, especially the introduction about :doc:`../first_steps/general_optimization_tasks`.

For this tutorial the following includes are needed::

..sharkcode<Supervised/FFNNBasicTutorial.tpp,includes>

Defining the Learning Problem
------------------------------
In this tutorial, we want to solve the infamous xor problem using a feed-forward network.
First, we define the problem by generating the training data. We consider two binary inputs
that are to be mapped to one if they have the same value and to zero otherwise.
Input patterns and corresponding target patterns are stored in a container for labeled data
after generation.

For this part, we need to include the Dataset and define the problem in a function, which
we will use shortly::

..sharkcode<Supervised/FFNNBasicTutorial.tpp,problem>

Defining the Network topology
------------------------------

After we have defined our Problem, we can define our feed forward
network. For this, we have to decide on the network topology. We have
to choose activation functions, how many hidden layers and
neurons we want, and how the layers are connected. This is quite a lot
of stuff, but Shark makes this task straight-forward.

The easiest part are the neurons. Shark offers several different types
of neurons named after their activation function  [ReedMarks1998]_:

* :doxy:`LogisticNeuron`: is a sigmoid (S-shaped)
  function with outputs in the range [0,1] and the following definition

.. math::
  f(x) = \frac 1 {1+e^{-x}}.

* :doxy:`TanhNeuron`: the hyperbolic tangens, can be viewed as a rescaled
  version of the Logistic function with outputs ranging from [-1,1]. It
  has the formula

.. math::
  f(x) = \tanh(x) = \frac 2 {1+e^{-2x}}-1.

* :doxy:`FastSigmoidNeuron`: a sigmoidal function which is faster to
  evaluate than the previous two activation functions. It has also "bigger
  tails" (i.e., the gradient does not vanish as quickly). This
  activation function is highly recommended and defined in Shark as

.. math::
  f(x) = \frac x {1+|x|}.
  
* :doxy:`RectifierNeuron`: rcent development where the neuron activation
  is kept at 0 for negative activation levels and linear for all positive values.
  A network with these neurons is effectively a piecewise linear function.

.. math::
  f(x) = max(0,x).

* :doxy:`LinearNeuron`: not a good choice for hidden neurons, but
  for output neurons when the output  is not bounded. This activation
  function :math:`f(x)=x` is the typical choice for regression tasks.

For our example, we will use the Logistic-Activation since it fits the labels
nicely. We choose the neuron types using two template parameters, one
for the hidden neurons, one for the visible. For the topology, we will
choose a network with 2 hidden neurons. The neurons are fully
connected (while obeying the layer structure). That is, we also allow
direct connections from the input to the output. We also want a bias
neuron (i.e., bias or offset parameters).
All this can be achieved with :doxy:`FFNet::setStructure`::

..sharkcode<Supervised/FFNNBasicTutorial.tpp,network_topology>

The method  :doxy:`FFNet::setStructure` is versatile. If we wanted a second hidden layer with 3 neurons, we could simply write::

  unsigned numInput=2;
  unsigned numHidden1=2;
  unsigned numHidden2=3;
  unsigned numOutput=1;
  network.setStructure(numInput, numHidden1, numHidden2, numOutput);

Training the Network
----------------------

After we have defined problem and topology, we can now finally train
the network. The most frequently used error function for
training neural networks is arguably the :doxy:`SquaredLoss`, but
Shark offers alternatives.
Since the xor Problem is a classification task, we can
use the :doxy:`NegativeClassificationLogLikelihood` error to maximize
the class probability  [Bishop1995]_. For optimizing this function the improved
resilient backpropagation algorithm ([IgelHüsken2003]_, a faster,
more robust variant of the seminal Rprop algorithm [Riedmiller1994]_) is used: ::

..sharkcode<Supervised/FFNNBasicTutorial.tpp,train>

If you don't know how to use and evaluate the trained model you will find the information in the getting started section.


Multiclass Training
----------------------
When training networks with more than two classes it is
not advisable anymore to use the ``NegativeClassificationLogLikelihood`` loss. It assumes
that every neuron (aside from the binary case) encodes one class, that all neurons
can only take values greater than zero and that the activation of all output neurons sums
to one, thus they need to form a proper probability :math:`p(c_i|x)` where :math:`c_i`
is the ith class and :math:`x` the current input.
A neural network typically does not perform normalisation in any sense. Thus in this
case the output must either be normalised, or the ``CrossEntropy`` is to be used instead.
The cross entropy uses an exponential normalisation under the assumption that the output
neurons of the network are linear and applies the maximum log likelihood principle on
the normalised values. This can also be achieved by using a proper normalisation model.


Other network types
---------------------

Shark offers many different types of neural other neural networks,
including radial basis function networks using :doxy:`RBFLayer`
and recurrent neural networks (:doxy:`RNNet`)
as well as support vector and regularization networks.

Full example program
----------------------

The full example program is  :doxy:`FFNNBasicTutorial.cpp`.
Th same example using a connection matrix to define the network
can be found at :doxy:`FFNNSetStructureTutorial.cpp`.
The following two files explain multi class classification with cross entropy:

Example with Cross Entropy :doxy:`FFNNMultiClassCrossEntropy.cpp`.
Normalised output  :doxy:`FFNNMultiClassNormalizedTraining.cpp`.

Both ways lead to the exact same results. Thus the second example explains the principle
of cross entropy learning.

References
^^^^^^^^^^

.. [Bishop1995] C.M. Bishop. Neural networks for pattern recognition. Oxford University Press, 1995.

.. [IgelHüsken2003] C. Igel and M. Hüsken.
   Empirical Evaluation of the Improved Rprop Learning Algorithm. Neurocomputing 50(C), pp. 105-123, 2003

.. [ReedMarks1998] R.D. Redd and R.J. Marks. Neural smithing:
   supervised learning in feedforward artificial neural networks. MIT  Press, 1998

.. [Riedmiller1994] M. Riedmiller.
   Advanced supervised learning in multilayer perceptrons-from backpropagation to adaptive learning techniques. International Journal of Computer Standards and Interfaces 16(3), pp. 265-278, 1994.
