<!DOCTYPE html>

<html lang="en" data-content_root="../../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    
    <title>Loss and Cost Functions &#8212; Shark 3.0a documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mt_sphinx_deriv.css?v=6ec4729f" />
    <script src="../../../../_static/documentation_options.js?v=db277b1a"></script>
    <script src="../../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML&delayStartupUntilConfig"></script>
    <link rel="icon" href="../../../../_static/shark16.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Optimizers" href="optimizers.html" />
    <link rel="prev" title="Kernels" href="kernels.html" />
    <link rel="stylesheet" href="../../../../_static/mt_sphinx_shark.css" type="text/css" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>
    <script src="../../../../index/../../../../mlstyle.js"></script>

  </head><body>

    <div id="shark_old">
        <div id="wrap">
            <div id="header">
                <div id="site-name"><a href="/shark/index.html">Shark machine learning library</a></div>
                <ul id="nav">
                    <li  class="first" >
                        <a href="../../../installation.html">Installation</a>
                    </li>
                    <li  class="active" >
                        <a href="../../tutorials.html">Tutorials</a>
                    </li>
		    <li  class="first" >
                        <a href="../../../benchmark.html">Benchmarks</a>
                    </li>
		    <li  class="first" >
                        <a href="../../../../index/../../../../doxygen_pages/html/classes.html">Documentation</a>
                        <ul>
                            <li><a href="../../../quickref/quickref.html">Quick references</a></li>
                            <li><a href="../../../../index/../../../../doxygen_pages/html/classes.html">Class list</a></li>
                            <li class="last"><a href="../../../../index/../../../../doxygen_pages/html/group__shark__globals.html">Global functions</a></li>
                        </ul>
                    </li>
                </ul>

            </div>
        </div>
    </div>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="loss-and-cost-functions">
<h1>Loss and Cost Functions<a class="headerlink" href="#loss-and-cost-functions" title="Link to this heading">¶</a></h1>
<p>Shark uses the notion of loss and cost functions to define machine
learning tasks.</p>
<section id="loss-functions">
<h2>Loss functions<a class="headerlink" href="#loss-functions" title="Link to this heading">¶</a></h2>
<p>Consider a model (a hypothesis) <span class="math notranslate nohighlight">\(f\)</span> mapping inputs <span class="math notranslate nohighlight">\(x\)</span>
to predictions <span class="math notranslate nohighlight">\(y=f(x)\in Y\)</span>.  Let <span class="math notranslate nohighlight">\(t\in Y\)</span> be the true
label of input pattern <span class="math notranslate nohighlight">\(x\)</span>.  Then a <em>loss function</em>
<span class="math notranslate nohighlight">\(L:Y\times Y\to\mathbb{R}^+_0\)</span> measures the quality of the
prediction. If the prediction is perfectly accurate, the loss function
is zero (<span class="math notranslate nohighlight">\(t=y\Rightarrow L(t, y)=0\)</span>). If not, the loss
function measures “how bad” the mistake is. The loss can be
interpreted as a penalty or error measure.</p>
<p>For a classification tast, a fundamental loss function
is the 0-1-loss:</p>
<div class="math notranslate nohighlight">
\[\begin{split}L(y,t)=\begin{cases} 0 &amp; \text{if $y=t$}\\1 &amp; \text{otherwise}\end{cases}\end{split}\]</div>
<p>For regression, the squared loss is most popular:</p>
<div class="math notranslate nohighlight">
\[L(y,t)= (y-t)^2\]</div>
<p>Using the concept of a loss function, the goal of supervised learning
can be described as finding a model <span class="math notranslate nohighlight">\(f\)</span> minimizing the <em>risk</em>:</p>
<div class="math notranslate nohighlight">
\[\mathcal{R}(f) = \mathbb{E}\{   L(t, f(x)) \}\]</div>
<p>Here the expectation <span class="math notranslate nohighlight">\(\mathbb{E}\)</span> is over the joint distribution
underlying the observations of inputs and corresponding labels.</p>
</section>
<section id="cost-functions">
<h2>Cost functions<a class="headerlink" href="#cost-functions" title="Link to this heading">¶</a></h2>
<p>Now let us consider a collection of observations
<span class="math notranslate nohighlight">\(S=\{(x_1,t_1),(x_2,t_2),\dots,(x_N,t_N)\}\in(X\times Y)^N\)</span> and
corresponding predictions <span class="math notranslate nohighlight">\(y_1.y_2,\dots,y_N\)</span>  by a model <span class="math notranslate nohighlight">\(f\)</span>.
A <em>cost function</em> <span class="math notranslate nohighlight">\(C\)</span> is a mapping assigning
an overall cost value, which can be interpreted as an overall error,
to <span class="math notranslate nohighlight">\(\{(y_1,t_1),(y_2,t_2),\dots,(y_N,t_N)\}\in(Y\times Y)^N\)</span>.
Every loss function induces a cost function, namely the <em>empirical
risk</em>:</p>
<div class="math notranslate nohighlight">
\[\mathcal{R}_S(f) = C(\{(y_1,t_1),(y_2,t_2),\dots,(y_N,t_N)\})  = \frac 1 N \sum_{i=1}^N L(y_i,t_i)\]</div>
<p>The cost function induced by the 0-1-loss is the average
misclassification error and the cost function induced by the squared
loss is the mean squared error (MSE).</p>
<p>However, there are cost functions which cannot be decomposed using a loss
function. For example, the <em>area under the curve</em> (AUC).
In other words, all loss functions generate a cost function, but not all cost
functions must be based on a loss function.</p>
</section>
<section id="list-of-classes">
<h2>List of Classes<a class="headerlink" href="#list-of-classes" title="Link to this heading">¶</a></h2>
<p>See the documentation for <a class="reference external" href="../../../../../../../doxygen_pages/html/group__lossfunctions.html">Loss functions</a> and <a class="reference external" href="../../../../../../../doxygen_pages/html/group__costfunctions.html">Cost functions</a>.</p>
<section id="derivatives">
<h3>Derivatives<a class="headerlink" href="#derivatives" title="Link to this heading">¶</a></h3>
<p>When both the loss function and the model are differentiable, it is possible
to calculate the derivative of the empirical risk with respect to the model
parameters <span class="math notranslate nohighlight">\(w\)</span>:</p>
<div class="math notranslate nohighlight">
\[\frac {\partial}{\partial w}\mathcal{R}_S(f)  = \frac 1 N \sum_{i=1}^N \frac {\partial}{\partial f(x_i)}L(f(x_i),t_i)\frac {\partial}{\partial w}f(x_i)\]</div>
<p>This allows embarrassingly parallelizable gradient descent on the cost
function. Please see the tutorial
<a class="reference internal" href="../optimization/conventions_derivatives.html"><span class="doc">Shark Conventions for Derivatives</span></a> for learning more about the
handling of derivatives in Shark.</p>
</section>
</section>
<section id="the-base-class-abstractcost-labeltypet-outputtypet">
<h2>The base class ‘AbstractCost&lt;LabelTypeT,OutputTypeT&gt;’<a class="headerlink" href="#the-base-class-abstractcost-labeltypet-outputtypet" title="Link to this heading">¶</a></h2>
<p>The base class <a class="reference external" href="../../../../../../../doxygen_pages/html/classshark_1_1_abstract_cost.html">AbstractCost</a> is templatized with respect to
both the label and output type.  Using batches, that is, collections
of input elements, is an important concept in Shark, see the tutorial
<a class="reference internal" href="batches.html"><span class="doc">Data Batches</span></a>. The proper batch types are inferred from the
label and output types:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Types</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>LabelType</p></td>
<td><p>Type of a label <span class="math notranslate nohighlight">\(t_i\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>OutputType</p></td>
<td><p>Type of a model output <span class="math notranslate nohighlight">\(z_i\)</span></p></td>
</tr>
<tr class="row-even"><td><p>BatchLabelType</p></td>
<td><p>Batch of Labels; same as Batch&lt;LabelType&gt;::type</p></td>
</tr>
<tr class="row-odd"><td><p>BatchOutputType</p></td>
<td><p>Batch of Outputs; same as Batch&lt;OutputType&gt;::type</p></td>
</tr>
</tbody>
</table>
<p>Like all other interfaces in Shark, cost functions have flags indicating their
internal capabilities:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Flag, Accessor function</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>HAS_FIRST_DERIVATIVE, hasFirstDerivative</p></td>
<td><p>Can the cost function calculate its first derivative?</p></td>
</tr>
<tr class="row-odd"><td><p>IS_LOSS_FUNCTION, isLossFunction</p></td>
<td><p>Is the cost function a loss
in the above terms (i.e., separable)?</p></td>
</tr>
</tbody>
</table>
<p>The interface of AbstractCost reflects the fact that costs can only be evaluated
on a complete set of data. The following functions can be used for evaluation of
<code class="docutils literal notranslate"><span class="pre">AbstractCost</span></code>. For brevity let <code class="docutils literal notranslate"><span class="pre">L</span></code> be the <code class="docutils literal notranslate"><span class="pre">LabelType</span></code> and <code class="docutils literal notranslate"><span class="pre">O</span></code> the
<code class="docutils literal notranslate"><span class="pre">OutputType</span></code>:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">double</span> <span class="pre">eval(Data&lt;L&gt;</span> <span class="pre">const&amp;</span> <span class="pre">label,</span> <span class="pre">Data&lt;O&gt;</span> <span class="pre">const&amp;</span> <span class="pre">predictions)</span></code></p></td>
<td><p>Returns the mean cost of the predictions <span class="math notranslate nohighlight">\(z_i\)</span> given the label
<span class="math notranslate nohighlight">\(t_i\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">double</span> <span class="pre">operator()(Data&lt;L&gt;</span> <span class="pre">const&amp;</span> <span class="pre">label,</span> <span class="pre">Data&lt;O&gt;</span> <span class="pre">const&amp;</span> <span class="pre">predictions)</span></code></p></td>
<td><p>Convenience function Returning eval(label,predictions)</p></td>
</tr>
</tbody>
</table>
</section>
<section id="the-base-class-abstractloss-labeltypet-outputtypet">
<h2>The base class ‘AbstractLoss&lt;LabelTypeT,OutputTypeT&gt;’<a class="headerlink" href="#the-base-class-abstractloss-labeltypet-outputtypet" title="Link to this heading">¶</a></h2>
<p>The base class <a class="reference external" href="../../../../../../../doxygen_pages/html/classshark_1_1_abstract_loss.html">AbstractLoss</a> is derived from AbstractCost. It implements
all methods of its base class and offers several additional methods. Shark code is
allowed to read the flag <code class="docutils literal notranslate"><span class="pre">IS_LOSS_FUNCTION</span></code> via the public method <code class="docutils literal notranslate"><span class="pre">isLossFunction()</span></code>
and to downcast an AbstractCost object to an AbstractLoss. This enables the use of the
following much more efficient interface:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Method</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">double</span> <span class="pre">eval(LabelType</span> <span class="pre">const&amp;</span> <span class="pre">t,</span> <span class="pre">InputType</span> <span class="pre">const&amp;</span> <span class="pre">z)</span></code></p></td>
<td><p>Returns the error of the prediction <span class="math notranslate nohighlight">\(z\)</span> given the label <span class="math notranslate nohighlight">\(t\)</span>.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">double</span> <span class="pre">eval(BatchLabelType</span> <span class="pre">const&amp;</span> <span class="pre">T,</span> <span class="pre">BatchInputType</span> <span class="pre">const&amp;</span> <span class="pre">Z)</span></code></p></td>
<td><p>Returns the sum of errors of the predictions <span class="math notranslate nohighlight">\(z_i \in Z\)</span> given the label <span class="math notranslate nohighlight">\(t_i \in T\)</span>.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">double</span> <span class="pre">operator()(LabelType</span> <span class="pre">const&amp;</span> <span class="pre">t,</span> <span class="pre">InputType</span> <span class="pre">const&amp;</span> <span class="pre">z)</span></code></p></td>
<td><p>Calls eval(t,z)</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">double</span> <span class="pre">operator()(BatchLabelType</span> <span class="pre">const&amp;</span> <span class="pre">T,</span> <span class="pre">BatchInputType</span> <span class="pre">const&amp;</span> <span class="pre">Z)</span></code></p></td>
<td><p>Calls eval(T,Z</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">double</span> <span class="pre">evalDerivative(BatchLabelType</span> <span class="pre">const&amp;</span> <span class="pre">T,</span> <span class="pre">BatchInputType</span> <span class="pre">const&amp;</span> <span class="pre">Z,</span> <span class="pre">BatchInputType</span> <span class="pre">const&amp;</span> <span class="pre">gradient)</span></code></p></td>
<td><p>Returns the error of the predictions <span class="math notranslate nohighlight">\(z_i\)</span> given the label <span class="math notranslate nohighlight">\(t_i\)</span>
and computes <span class="math notranslate nohighlight">\(\frac {\partial}{\partial z_i}L(z_i,t_i)\)</span></p></td>
</tr>
</tbody>
</table>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
	<div class="mt_ltocwrapper">
		<ul>
<li><a class="reference internal" href="#">Loss and Cost Functions</a><ul>
<li><a class="reference internal" href="#loss-functions">Loss functions</a></li>
<li><a class="reference internal" href="#cost-functions">Cost functions</a></li>
<li><a class="reference internal" href="#list-of-classes">List of Classes</a></li>
<li><a class="reference internal" href="#the-base-class-abstractcost-labeltypet-outputtypet">The base class ‘AbstractCost&lt;LabelTypeT,OutputTypeT&gt;’</a></li>
<li><a class="reference internal" href="#the-base-class-abstractloss-labeltypet-outputtypet">The base class ‘AbstractLoss&lt;LabelTypeT,OutputTypeT&gt;’</a></li>
</ul>
</li>
</ul>

	</div>
<div>
  <a class="topless" href="kernels.html" title="previous chapter">
	  <img class="navicon" src="../../../../_static/icon_backward.png" alt="prev"/> Kernels</a>
  <a class="topless" href="optimizers.html" title="next chapter">
	  <img class="navicon" src="../../../../_static/icon_forward.png" alt="next"/> Optimizers</a>
</div> 
<div id="searchbox" style="display: none">
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" size="12" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
      <input class="mtsubmitbutton" type="submit" value="Find" />
    </form>
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
<p class="mtshowsource">
  <a href="../../../../_sources/rest_sources/tutorials/concepts/library_design/losses.rst.txt"
           rel="nofollow"><img class="sourceicon" src="../../../../_static/icon_eject.png" alt="prev"/> Show page source</a>
</p>
        </div>
      </div>
      <div class="clearer"></div>
    </div>

    <div class="footer">
        <div class="footerlogos">
            <a href="http://validator.w3.org/check/referer" title="Valid XHTML 1.0">
                <img class="footerlogos" src="../../../../_static/xhtml_validation.png" alt="Valid XHTML 1.0" />
            </a>
            <a href="http://jigsaw.w3.org/css-validator/check/referer?profile=css3" title="Valid CSS3">
                <img class="footerlogos" src="../../../../_static/css_validation.png" alt="Valid CSS3" />
            </a>
        </div>
            &copy; The Shark developer team.
           Created on 21/05/2024
           using <a href="http://sphinx.pocoo.org/">Sphinx</a> 7.3.7
    </div>
  </body>
</html>