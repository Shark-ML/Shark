<!DOCTYPE html>

<html lang="en" data-content_root="../../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    
    <title>Shark Conventions for Derivatives &#8212; Shark 3.0a documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mt_sphinx_deriv.css?v=6ec4729f" />
    <script src="../../../../_static/documentation_options.js?v=db277b1a"></script>
    <script src="../../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML&delayStartupUntilConfig"></script>
    <link rel="icon" href="../../../../_static/shark16.ico"/>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <link rel="next" title="Iterative Calculation of Statistics" href="../misc/statistics.html" />
    <link rel="prev" title="Direct Search Algorithms" href="directsearch.html" />
    <link rel="stylesheet" href="../../../../_static/mt_sphinx_shark.css" type="text/css" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../../../../index/../../../../mlstyle.js"></script>

  </head><body>

    <div id="shark_old">
        <div id="wrap">
            <div id="header">
                <div id="site-name"><a href="/shark/index.html">Shark machine learning library</a></div>
                <ul id="nav">
                    <li  class="first" >
                        <a href="../../../installation.html">Installation</a>
                    </li>
                    <li  class="active" >
                        <a href="../../tutorials.html">Tutorials</a>
                    </li>
		    <li  class="first" >
                        <a href="../../../benchmark.html">Benchmarks</a>
                    </li>
		    <li  class="first" >
                        <a href="../../../../index/../../../../doxygen_pages/html/classes.html">Documentation</a>
                        <ul>
                            <li><a href="../../../quickref/quickref.html">Quick references</a></li>
                            <li><a href="../../../../index/../../../../doxygen_pages/html/classes.html">Class list</a></li>
                            <li class="last"><a href="../../../../index/../../../../doxygen_pages/html/group__shark__globals.html">Global functions</a></li>
                        </ul>
                    </li>
                </ul>

            </div>
        </div>
    </div>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="shark-conventions-for-derivatives">
<h1>Shark Conventions for Derivatives<a class="headerlink" href="#shark-conventions-for-derivatives" title="Link to this heading">¶</a></h1>
<section id="background">
<h2>Background<a class="headerlink" href="#background" title="Link to this heading">¶</a></h2>
<p>Gradient-based optimization is the most common reason for computing
derivatives in Shark. Gradient-based optimizers operate on objective
functions, however, these will often delegate the job further on to
models, kernels, and loss or more general cost functions. Thus, all
these types of objects have the capability to compute derivatives
foreseen in their respective interfaces. However, getting the best
performance when evaluating derivatives is not always straightforward.
Since Shark aims for maximal speed, the library enforces a very
specific evaluation scheme for derivative computations. The design
rationale will be explained in the following.</p>
</section>
<section id="an-example-the-derivative-of-the-error">
<h2>An Example: The Derivative of the Error<a class="headerlink" href="#an-example-the-derivative-of-the-error" title="Link to this heading">¶</a></h2>
<p>Let’s consider a simple example, namely the derivative of a squared
error term</p>
<div class="math notranslate nohighlight">
\[E = \frac 1 N \sum_{i=1}^N L(f_w(x_i), t_i)\]</div>
<p>w.r.t. a parameter <span class="math notranslate nohighlight">\(w_i\)</span> of a parametric family <span class="math notranslate nohighlight">\(f_w(x)\)</span>
of models, evaluated with the squared loss <span class="math notranslate nohighlight">\(L(y, t) = (y-t)^2\)</span>:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\frac{\partial E}{\partial w_k} &amp;= \frac 1 N \sum_{i=1}^N L(f_w(x_i), t_i)\\                                &amp;= \frac 1 N \sum_{i=1}^N \frac{\partial L}{\partial y}(f_w(x_i), t_i) \frac{\partial f_w}{\partial w_k}(x_i)\\                                &amp;= \sum_{i=1}^N \frac 2 N (f_w(x_i) - t_i) \frac{\partial f_w}{\partial w}(x_i)\end{aligned}\end{align} \]</div>
<p>The derivative involves the chain rule for the combination of model
and loss. The term can be understood as a weighted sum of the partial
model derivatives, where the weights are the loss derivatives. Note
how these weights do not require the model derivatives, but they do
depend on the model’s output values. This is the general situation
when chaining computations:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial f \circ g(x)}{\partial w_k} = f'(g(x)) g'(x)\]</div>
<p>The value <span class="math notranslate nohighlight">\(g(x)\)</span> is needed as the point in which the derivative
<span class="math notranslate nohighlight">\(f'\)</span> is to be evaluated, and it is used rather independent of
the derivative <span class="math notranslate nohighlight">\(g'(x)\)</span>.</p>
<p>In a typical error function the overall derivative is a weighted sum of
model derivatives, evaluated in different points. The weights require
only the model evaluations in these points, not their derivatives. This
hints at the following order of evaluation:</p>
<blockquote>
<div><ul class="simple">
<li><p>evaluate the model values <span class="math notranslate nohighlight">\(y_i = f_w(x_i)\)</span>,</p></li>
<li><p>evaluate the loss derivatives <span class="math notranslate nohighlight">\(\frac{\partial L}{\partial y}(y_i, t_i)\)</span>,</p></li>
<li><p>evaluate the model derivatives <span class="math notranslate nohighlight">\(\frac{\partial f}{\partial w_i}(x_i)\)</span>
and compute their weighted sum.</p></li>
</ul>
</div></blockquote>
</section>
<section id="two-stage-derivative-computation">
<h2>Two-stage Derivative Computation<a class="headerlink" href="#two-stage-derivative-computation" title="Link to this heading">¶</a></h2>
<p>The order of computation as laid out above is a necessity
for the efficient evaluation of derivatives of chains. We lift this necessity
to a principle: <cite>first evaluate, then differentiate</cite>. In other words,
always call <cite>eval</cite> on an object before calling <cite>evalDerivative</cite>
or similar functions. Otherwise the results of the derivative are
undefined. This holds for models and kernels. Objective functions and losses
can compute both at once since they can be interpreted as the ends of the chain
of computation - the loss is required to evaluate and return the full derivative
while the objective function returns the final summed result.</p>
<p>In simple situations the order of evaluation is not crucial. However,
in general the requirement to evaluate before computing derivatives is
not restrictive at all. So even if there is no natural order of calls,
the order is dictated by the Shark interface design.</p>
<p>The rationale behind this design is that there are often strong
synergies between the computation of the value and its derivative.
More often than not, the derivative is a cheap byproduct once the
value has been computed. Thus, efficient evaluation of both the value
and the derivative requires either the computation of both at the same
time, or the storage of intermediate results. The first way is not
viable in case of chained computations. Therefore Shark has decided to
take the second route, and to store intermediate values for derivative
computations in the object. This state is written by the evaluation
method and read by the derivative computations.</p>
</section>
<section id="another-example-the-derivative-of-a-concatenation-of-models">
<h2>Another Example: The derivative of a concatenation of models<a class="headerlink" href="#another-example-the-derivative-of-a-concatenation-of-models" title="Link to this heading">¶</a></h2>
<p>Let’s assume the function <span class="math notranslate nohighlight">\(f_w\)</span> of the previous example would in fact
not be a single model, but two models where the output of the first model is
the input of the second, such that with <span class="math notranslate nohighlight">\(w=(u,v)\)</span> being the combined parameter
vector of both models, we get <span class="math notranslate nohighlight">\(f_w(x)=g_u(h_v(x))\)</span>.Thus the derivatives are:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial E}{\partial u_k}f_w(x) =\frac{\partial g_u}{\partial u_k}(h_v(x))\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\frac{\partial E}{\partial v_k}f_w(x)
= \frac{\partial g_u}{\partial h_v(x)}(h_v(x)) \frac{\partial h_v}{\partial v_k} h_v(x)\]</div>
<p>Please remember that the partial derivatives with respect to the arguments of <span class="math notranslate nohighlight">\(g\)</span> are
full jacobi matrices and not single values or vectors. Thus the computation of <span class="math notranslate nohighlight">\(v_k\)</span>
as stated here requires a matrix-vector product for every parameter <span class="math notranslate nohighlight">\(v_k\)</span>, or a
matrix-matrix product if the derivative is computed for all <span class="math notranslate nohighlight">\(v_k\)</span> at once.
But putting this into the the equation of the derivative of the error function of the
previous example, we  get for the derivative with respect to <span class="math notranslate nohighlight">\(v_k\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial E}{\partial v_k}
&amp;= \sum_{i=1}^N \frac 2 N (f_w(x_i) - t_i) \frac{\partial f_w}{\partial v_k}(x_i)\\
&amp;= \sum_{i=1}^N \frac 2 N (f_w(x_i) - t_i) \frac{\partial g_u}{\partial h_v(x_i)}(h_v(x_i)) \frac{\partial h_v}{\partial v_k} h_v(x_i)\end{split}\]</div>
<p>now adding braces around the derivative of the loss and the partial derivative of <span class="math notranslate nohighlight">\(g_u\)</span>
we see that this term can be computed as matrix-vector product. Thus the whole Term can be
computed using 2N matrix-vector products instead of N matrix-matrix and matrix-vector products!
This makes in practice a huge difference.</p>
</section>
<section id="weighted-sums-of-derivatives">
<h2>Weighted Sums of Derivatives<a class="headerlink" href="#weighted-sums-of-derivatives" title="Link to this heading">¶</a></h2>
<p>In the first example, the derivative of the squared error w.r.t. a model
parameter is a weighted sum of derivatives for single data points. The
computation of the weights requires the model’s output values for the
same data points. Again, this situation is completely general, and thus
Shark makes it a principle: <cite>derivatives are returned as weighted sums</cite>.</p>
<p>A single call to a derivative function may evaluate the derivative in a
whole batch or even in a whole data set of different points. However, in
the next processing stage these values will typically all enter the same
cost function. Thus, the derivative is a weighted sum, with the cost
derivatives being the weights.</p>
<p>Now for chaining of the derivatives as in the second example, we can first
evaluate the weighted derivative with respects to the inputs of <span class="math notranslate nohighlight">\(g\)</span>,
which amounts to computing the aforementioned bracing. After that the resulting
vector can be used to calculate the weighted derivative of <span class="math notranslate nohighlight">\(h\)</span> with
respect to it’s parameters. We can further optimize this scheme by
computing both derivatives of <span class="math notranslate nohighlight">\(g\)</span> at the same time using that again
in many cases input and parameter derivative can share a lot of computations.</p>
<p>A well known example which uses both optimizations weighted sum calculation
and shared computation of derivatives is the back-propagation of error algorithm,
which not only allows for a more efficient computation in terms of the complexity
of the algorithm, but also allow for a more efficient optimization
for RAM throughput, etc. To check that this is in fact the same algorithm, define
<span class="math notranslate nohighlight">\(g\)</span> and <span class="math notranslate nohighlight">\(h\)</span> as neuron layers of a three layer neural network.</p>
<p>Thus Shark’s derivative interfaces can be understood as a generalization of
the same computational trick. The exact weighting scheme applied slightly
varies across the different interfaces, e.g., models versus kernels.</p>
</section>
<section id="batching-derivatives-and-how-to-derive-them">
<h2>Batching Derivatives and how to derive them<a class="headerlink" href="#batching-derivatives-and-how-to-derive-them" title="Link to this heading">¶</a></h2>
<p>As previously mentioned in short, batching is also applied to derivatives. The net
effect of batch computing is not as dramatic for the computation time as the application
of weighted derivatives, but still quite significant. However, deriving efficient batched
computations of weighted derivatives is not straight forward and we are constantly trying
to improve the results.
In the case of the parameter derivative for example the input is a matrix of values:
every row consists of one weight for every output and each row represents one sample.
Computing this derivative naiively, the result would be a three tensor which
we need to reduce to a single vector by summing over two dimensions. Thus choosing the
order in which this reduction is performed - preferently without actually calculating
the big tensor itself - can make a huge difference. The key to success in any case is
to use matrix notation wherever possible instead of using elementwise derivations
as is often done on a sheet of paper. While Vector and Matrix calculus sems unfamiliar at
first glance, it immediately answers the questions about which computations can be grouped
together and which efficient linear algebra operations can be used.</p>
<div class="admonition-todo admonition" id="id1">
<p class="admonition-title">Todo</p>
<p>TG: Present one simple and one involved use case? Or is this the wrong place?</p>
<p>OK: We need one tutorial for Kernels and Models which explain how these
derivatives can actually b calculated. Maybe introduce your nice scalar
product syntax which makes the calculation a breese.</p>
</div>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
	<div class="mt_ltocwrapper">
		<ul>
<li><a class="reference internal" href="#">Shark Conventions for Derivatives</a><ul>
<li><a class="reference internal" href="#background">Background</a></li>
<li><a class="reference internal" href="#an-example-the-derivative-of-the-error">An Example: The Derivative of the Error</a></li>
<li><a class="reference internal" href="#two-stage-derivative-computation">Two-stage Derivative Computation</a></li>
<li><a class="reference internal" href="#another-example-the-derivative-of-a-concatenation-of-models">Another Example: The derivative of a concatenation of models</a></li>
<li><a class="reference internal" href="#weighted-sums-of-derivatives">Weighted Sums of Derivatives</a></li>
<li><a class="reference internal" href="#batching-derivatives-and-how-to-derive-them">Batching Derivatives and how to derive them</a></li>
</ul>
</li>
</ul>

	</div>
<div>
  <a class="topless" href="directsearch.html" title="previous chapter">
	  <img class="navicon" src="../../../../_static/icon_backward.png" alt="prev"/> Direct Search Algorithms</a>
  <a class="topless" href="../misc/statistics.html" title="next chapter">
	  <img class="navicon" src="../../../../_static/icon_forward.png" alt="next"/> Iterative Calculation of Statistics</a>
</div> 
<div id="searchbox" style="display: none">
    <form class="search" action="../../../../search.html" method="get">
      <input type="text" name="q" size="12" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
      <input class="mtsubmitbutton" type="submit" value="Find" />
    </form>
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
<p class="mtshowsource">
  <a href="../../../../_sources/rest_sources/tutorials/concepts/optimization/conventions_derivatives.rst.txt"
           rel="nofollow"><img class="sourceicon" src="../../../../_static/icon_eject.png" alt="prev"/> Show page source</a>
</p>
        </div>
      </div>
      <div class="clearer"></div>
    </div>

    <div class="footer">
        <div class="footerlogos">
            <a href="http://validator.w3.org/check/referer" title="Valid XHTML 1.0">
                <img class="footerlogos" src="../../../../_static/xhtml_validation.png" alt="Valid XHTML 1.0" />
            </a>
            <a href="http://jigsaw.w3.org/css-validator/check/referer?profile=css3" title="Valid CSS3">
                <img class="footerlogos" src="../../../../_static/css_validation.png" alt="Valid CSS3" />
            </a>
        </div>
            &copy; The Shark developer team.
           Created on 21/05/2024
           using <a href="http://sphinx.pocoo.org/">Sphinx</a> 7.3.7
    </div>
  </body>
</html>