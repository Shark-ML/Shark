<!DOCTYPE html>

<html lang="en" data-content_root="../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    
    <title>Kernelized Budgeted Stochastic Gradient Descent &#8212; Shark 3.0a documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mt_sphinx_deriv.css?v=6ec4729f" />
    <script src="../../../_static/documentation_options.js?v=db277b1a"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML&delayStartupUntilConfig"></script>
    <link rel="icon" href="../../../_static/shark16.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Covariance Matrix Adaptation Evolution Strategy" href="cma.html" />
    <link rel="prev" title="Kernel Target Alignment" href="kta.html" />
    <link rel="stylesheet" href="../../../_static/mt_sphinx_shark.css" type="text/css" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js?config=TeX-AMS_CHTML"></script>
    <script src="../../../index/../../../../mlstyle.js"></script>

  </head><body>

    <div id="shark_old">
        <div id="wrap">
            <div id="header">
                <div id="site-name"><a href="../../../index/../../../../index.html">Shark machine learning library</a></div>
                <ul id="nav">
                    <li  class="first" >
                        <a href="../../installation.html">Installation</a>
                    </li>
                    <li  class="active" >
                        <a href="../tutorials.html">Tutorials</a>
                    </li>
		    <li  class="first" >
                        <a href="../../benchmark.html">Benchmarks</a>
                    </li>
		    <li  class="first" >
                        <a href="../../../index/../../../../doxygen_pages/html/classes.html">Documentation</a>
                        <ul>
                            <li><a href="../../quickref/quickref.html">Quick references</a></li>
                            <li><a href="../../../index/../../../../doxygen_pages/html/classes.html">Class list</a></li>
                            <li class="last"><a href="../../../index/../../../../doxygen_pages/html/group__shark__globals.html">Global functions</a></li>
                        </ul>
                    </li>
                </ul>

            </div>
        </div>
    </div>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="kernelized-budgeted-stochastic-gradient-descent">
<h1>Kernelized Budgeted Stochastic Gradient Descent<a class="headerlink" href="#kernelized-budgeted-stochastic-gradient-descent" title="Link to this heading">¶</a></h1>
<p>Support vector machines and other kernel-based learning algorithms
are widely used and have many benefits. They can be considered as
state-of-the-art algorithms in machine learning. Despite being easy
to use, for larger data sets the kernelization, which was central to
the development of SVM, becomes a bottleneck, as the computation
time grows quadratically in the number of support vector– but the
latter have been shown to grow linearly in the dataset size.
Therefore the whole training process becomes quadratically,
and impractical for even remotely large datasets.
This problem was called the curse of kernelization in <a class="reference internal" href="#wangcrammervucetic2012" id="id1"><span>[WangCrammerVucetic2012]</span></a>.</p>
<p>There are different ways to solve this problem.
One intuitive method was presented in
[WangCrammerVucetic2012]. The idea is to put a constraint
on the complexity of the model, i.e. the sparsity of the weight vector.
As the weight vector in features space is a sum of basis functions,
this means that it has to have the form <span class="math notranslate nohighlight">\(w = \sum_{i=1}^B k(x_i, \cdot)\)</span>,
where B is the chosen budget size and <span class="math notranslate nohighlight">\(x_i\)</span> are some
data points.</p>
<p>[WangCrammerVucetic2012] employ a well-known
stochastic gradient descent method, Pegasos, to train the model in a
perceptron-like fashion:
In each round the algorithms is given a data point.
If it violates the margin with respect to the
current model (so the example is either
classified incorrectly or with a too low confidence), it will be added
to the weight vector, also called budget, just like in Pegasos.</p>
<p>Obviously, at some point the budget becomes full.
In this case, adding a new vector will violate the  size-constraint.
Therefore one needs a way to reduce the size of the budget.
These, often heuristic, methods are called budget maintenence strategies.
Many such strategies exist. One of the easiest is just to remove
randomly a vector from the budget. Another strategy is remove the
‘oldest’ vector (this method is called Forgetron).
Both strategies maintain the budget size, but are not optimal
in a certain sense, as they do not really try to minimize the
degradation of the model that occurs when one removes a
support vector. A better way was proposed in [WangCrammerVucetic2012]:
The idea is to find a pair of vectors that, when merged into one vector,
does degrade the quality of the solution as low as possible.
This can be formulated as
an optimization problem, and it can be shown that with a heuristic
search for such a pair, training is much better than with a
random maintenence strategy.</p>
<p>In Shark both strategies,  the remove and the merge strategy, can be applied.
Tthis tutorial shows how to use the Kernelized Budgeted SGD Trainer
in Shark with the merge strategy.</p>
<section id="kernelbudgetedsgd-in-shark">
<h2>KernelBudgetedSGD in  Shark<a class="headerlink" href="#kernelbudgetedsgd-in-shark" title="Link to this heading">¶</a></h2>
<p>For this tutorial the following include files are needed:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;shark/Algorithms/Trainers/Budgeted/KernelBudgetedSGDTrainer.h&gt;</span><span class="c1"> // the KernelBudgetedSGD trainer</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;shark/Algorithms/Trainers/Budgeted/MergeBudgetMaintenanceStrategy.h&gt;</span><span class="c1"> // the strategy the trainer will use</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;shark/Data/DataDistribution.h&gt;</span><span class="c1"> //includes small toy distributions</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;shark/Models/Kernels/GaussianRbfKernel.h&gt;</span><span class="c1"> //the used kernel for the SVM</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;shark/ObjectiveFunctions/Loss/HingeLoss.h&gt;</span><span class="c1"> // the loss we want to use for the SGD machine</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;shark/ObjectiveFunctions/Loss/ZeroOneLoss.h&gt;</span><span class="c1"> //used for evaluation of the classifier</span>
</pre></div>
</div>
<section id="toy-problem">
<h3>Toy problem<a class="headerlink" href="#toy-problem" title="Link to this heading">¶</a></h3>
<p>In this tutorial, we consider the chessboard problem, which is a well-known
artificial binary benchmark classification problem:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">unsigned</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">ell</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">500</span><span class="p">;</span><span class="w">     </span><span class="c1">// number of training data point</span>
<span class="kt">unsigned</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">tests</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">10000</span><span class="p">;</span><span class="w"> </span><span class="c1">// number of test data points</span>

<span class="n">Chessboard</span><span class="w"> </span><span class="n">problem</span><span class="p">;</span><span class="w"> </span><span class="c1">// artificial benchmark data</span>
<span class="n">ClassificationDataset</span><span class="w"> </span><span class="n">trainingData</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">problem</span><span class="p">.</span><span class="n">generateDataset</span><span class="p">(</span><span class="n">ell</span><span class="p">);</span>
<span class="n">ClassificationDataset</span><span class="w"> </span><span class="n">testData</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">problem</span><span class="p">.</span><span class="n">generateDataset</span><span class="p">(</span><span class="n">tests</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="model-and-learning-algorithm">
<h3>Model and learning algorithm<a class="headerlink" href="#model-and-learning-algorithm" title="Link to this heading">¶</a></h3>
<p>The steps to use the KernelBudgetedSGD trainer are the very same
one uses to build a CSvmTrainer <a class="reference external" href="../../../../../../doxygen_pages/html/classshark_1_1_c_svm_trainer.html">CSvmTrainer</a>. Thus,
to build our trainer, we need a <a class="reference external" href="../../../../../../doxygen_pages/html/structshark_1_1_kernel_classifier.html">KernelClassifier</a>  and an
<a class="reference external" href="../../../../../../doxygen_pages/html/classshark_1_1_kernel_budgeted_s_g_d_trainer.html">KernelBudgetedSGDTrainer</a>.</p>
<p>Our model is given by the two components: A
standard Gaussian/RBF kernel, which we create as usual:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">double</span><span class="w"> </span><span class="n">gamma</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">0.5</span><span class="p">;</span><span class="w">         </span><span class="c1">// kernel bandwidth parameter</span>

<span class="n">GaussianRbfKernel</span><span class="o">&lt;&gt;</span><span class="w"> </span><span class="n">kernel</span><span class="p">(</span><span class="n">gamma</span><span class="p">);</span><span class="w"> </span><span class="c1">// Gaussian kernel</span>
</pre></div>
</div>
<p>and a kernel classifier:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">KernelClassifier</span><span class="o">&lt;</span><span class="n">RealVector</span><span class="o">&gt;</span><span class="w"> </span><span class="n">kernelClassifier</span><span class="p">;</span><span class="w"> </span><span class="c1">// (affine) linear function in kernel-induced feature space</span>
</pre></div>
</div>
<p>Then, training the machine is simply performed by calling:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">double</span><span class="w"> </span><span class="n">C</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0</span><span class="p">;</span><span class="w">          </span><span class="c1">// regularization parameter</span>
<span class="kt">bool</span><span class="w"> </span><span class="n">bias</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">false</span><span class="p">;</span><span class="w">           </span><span class="c1">// use bias/offset parameter</span>
<span class="kt">size_t</span><span class="w"> </span><span class="n">budgetSize</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">16</span><span class="p">;</span><span class="w">     </span><span class="c1">// our model shall contain at most 16 vectors</span>
<span class="kt">size_t</span><span class="w"> </span><span class="n">epochs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">5</span><span class="p">;</span><span class="w">      </span><span class="c1">// we want to run 5 epochs</span>

<span class="n">HingeLoss</span><span class="w"> </span><span class="n">hingeLoss</span><span class="p">;</span><span class="w"> </span><span class="c1">// define the loss we want to use while training</span>
<span class="c1">// as the budget maintenance strategy we choose the merge strategy</span>
<span class="n">MergeBudgetMaintenanceStrategy</span><span class="o">&lt;</span><span class="n">RealVector</span><span class="o">&gt;</span><span class="w"> </span><span class="o">*</span><span class="n">strategy</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">new</span><span class="w"> </span><span class="n">MergeBudgetMaintenanceStrategy</span><span class="o">&lt;</span><span class="n">RealVector</span><span class="o">&gt;</span><span class="p">();</span>
<span class="n">KernelBudgetedSGDTrainer</span><span class="o">&lt;</span><span class="n">RealVector</span><span class="o">&gt;</span><span class="w"> </span><span class="n">kernelBudgetedSGDtrainer</span><span class="p">(</span><span class="o">&amp;</span><span class="n">kernel</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">hingeLoss</span><span class="p">,</span><span class="w"> </span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="n">bias</span><span class="p">,</span><span class="w"> </span><span class="nb">false</span><span class="p">,</span><span class="w"> </span><span class="n">budgetSize</span><span class="p">,</span><span class="w"> </span><span class="n">strategy</span><span class="p">);</span><span class="w">        </span><span class="c1">// create the trainer</span>
<span class="n">kernelBudgetedSGDtrainer</span><span class="p">.</span><span class="n">setEpochs</span><span class="p">(</span><span class="n">epochs</span><span class="p">);</span><span class="w">      </span><span class="c1">// set the epochs number</span>
</pre></div>
</div>
<p>As in the <a class="reference external" href="../../../../../../doxygen_pages/html/classshark_1_1_c_svm_trainer.html">CSvmTrainer</a>, the parameter  <code class="docutils literal notranslate"><span class="pre">C</span></code> denotes the
regularization parameter (the SVM uses the 1-norm
penalty for target margin violations by default) and <cite>bias</cite> the inclusion of a bias term in the solver..</p>
</section>
<section id="evaluating-the-model">
<h3>Evaluating the model<a class="headerlink" href="#evaluating-the-model" title="Link to this heading">¶</a></h3>
<p>To evaluate the model, we simply create a test dataset by generating
another chessboard problem. We can evaluate our trained model
on the test data set as well as the train dataset (the latter one just to
get a feeling how good the training went and to see overfitting problems).
We consider the standard 0-1 loss as performance measure. The code
then reads:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">ZeroOneLoss</span><span class="o">&lt;</span><span class="kt">unsigned</span><span class="w"> </span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">loss</span><span class="p">;</span><span class="w"> </span><span class="c1">// 0-1 loss</span>
<span class="n">Data</span><span class="o">&lt;</span><span class="kt">unsigned</span><span class="w"> </span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kernelClassifier</span><span class="p">(</span><span class="n">trainingData</span><span class="p">.</span><span class="n">inputs</span><span class="p">());</span><span class="w"> </span><span class="c1">// evaluate on training set</span>
<span class="kt">double</span><span class="w"> </span><span class="n">train_error</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">loss</span><span class="p">.</span><span class="n">eval</span><span class="p">(</span><span class="n">trainingData</span><span class="p">.</span><span class="n">labels</span><span class="p">(),</span><span class="w"> </span><span class="n">output</span><span class="p">);</span>
<span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;training error:</span><span class="se">\t</span><span class="s">&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w">  </span><span class="n">train_error</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="p">;</span>
<span class="n">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kernelClassifier</span><span class="p">(</span><span class="n">testData</span><span class="p">.</span><span class="n">inputs</span><span class="p">());</span><span class="w"> </span><span class="c1">// evaluate on test set</span>
<span class="kt">double</span><span class="w"> </span><span class="n">test_error</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">loss</span><span class="p">.</span><span class="n">eval</span><span class="p">(</span><span class="n">testData</span><span class="p">.</span><span class="n">labels</span><span class="p">(),</span><span class="w"> </span><span class="n">output</span><span class="p">);</span>
<span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;test error:</span><span class="se">\t</span><span class="s">&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">test_error</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
</section>
</section>
<section id="full-example-program">
<h2>Full example program<a class="headerlink" href="#full-example-program" title="Link to this heading">¶</a></h2>
<p>The full example program considered in this tutorial is <a class="reference external" href="../../../../../../doxygen_pages/html/_kernel_budgeted_s_g_d_tutorial_8cpp.html">KernelBudgetedSGDTutorial.cpp</a>.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">¶</a></h2>
<div role="list" class="citation-list">
<div class="citation" id="wangcrammervucetic2012" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">WangCrammerVucetic2012</a><span class="fn-bracket">]</span></span>
<ol class="upperalpha simple" start="26">
<li><p>Wang, K. Crammer and S. Vucetic: Breaking the curse of kernelization: Budgeted stochastic gradient descent for large-scale SVM training. The Journal of Machine Learning Research 13.1 (2012): 3103-3131.</p></li>
</ol>
</div>
</div>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
	<div class="mt_ltocwrapper">
		<ul>
<li><a class="reference internal" href="#">Kernelized Budgeted Stochastic Gradient Descent</a><ul>
<li><a class="reference internal" href="#kernelbudgetedsgd-in-shark">KernelBudgetedSGD in  Shark</a></li>
<li><a class="reference internal" href="#full-example-program">Full example program</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

	</div>
<div>
  <a class="topless" href="kta.html" title="previous chapter">
	  <img class="navicon" src="../../../_static/icon_backward.png" alt="prev"/> Kernel Target Alignment</a>
  <a class="topless" href="cma.html" title="next chapter">
	  <img class="navicon" src="../../../_static/icon_forward.png" alt="next"/> Covariance Matrix Adaptation Evolution Strategy</a>
</div> 
<div id="searchbox" style="display: none">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" size="12" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
      <input class="mtsubmitbutton" type="submit" value="Find" />
    </form>
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
<p class="mtshowsource">
  <a href="../../../_sources/rest_sources/tutorials/algorithms/kernelBudgetedSGD.rst.txt"
           rel="nofollow"><img class="sourceicon" src="../../../_static/icon_eject.png" alt="prev"/> Show page source</a>
</p>
        </div>
      </div>
      <div class="clearer"></div>
    </div>

    <div class="footer">
        <div class="footerlogos">
            <a href="http://validator.w3.org/check/referer" title="Valid XHTML 1.0">
                <img class="footerlogos" src="../../../_static/xhtml_validation.png" alt="Valid XHTML 1.0" />
            </a>
            <a href="http://jigsaw.w3.org/css-validator/check/referer?profile=css3" title="Valid CSS3">
                <img class="footerlogos" src="../../../_static/css_validation.png" alt="Valid CSS3" />
            </a>
        </div>
            &copy; The Shark developer team.
           Created on 22/05/2024
           using <a href="http://sphinx.pocoo.org/">Sphinx</a> 7.3.7
    </div>
  </body>
</html>