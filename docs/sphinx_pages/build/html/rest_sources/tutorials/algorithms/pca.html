<!DOCTYPE html>

<html lang="en" data-content_root="../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    
    <title>Principal Component Analysis &#8212; Shark 3.0a documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mt_sphinx_deriv.css?v=6ec4729f" />
    <script src="../../../_static/documentation_options.js?v=db277b1a"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML&delayStartupUntilConfig"></script>
    <link rel="icon" href="../../../_static/shark16.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Linear Discriminant Analysis" href="lda.html" />
    <link rel="prev" title="Serialization" href="../concepts/misc/serialization.html" />
    <link rel="stylesheet" href="../../../_static/mt_sphinx_shark.css" type="text/css" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="../../../index/../../../../mlstyle.js"></script>

  </head><body>

    <div id="shark_old">
        <div id="wrap">
            <div id="header">
                <div id="site-name"><a href="/shark/index.html">Shark machine learning library</a></div>
                <ul id="nav">
                    <li  class="first" >
                        <a href="../../installation.html">Installation</a>
                    </li>
                    <li  class="active" >
                        <a href="../tutorials.html">Tutorials</a>
                    </li>
		    <li  class="first" >
                        <a href="../../benchmark.html">Benchmarks</a>
                    </li>
		    <li  class="first" >
                        <a href="../../../index/../../../../doxygen_pages/html/classes.html">Documentation</a>
                        <ul>
                            <li><a href="../../quickref/quickref.html">Quick references</a></li>
                            <li><a href="../../../index/../../../../doxygen_pages/html/classes.html">Class list</a></li>
                            <li class="last"><a href="../../../index/../../../../doxygen_pages/html/group__shark__globals.html">Global functions</a></li>
                        </ul>
                    </li>
                </ul>

            </div>
        </div>
    </div>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="principal-component-analysis">
<h1>Principal Component Analysis<a class="headerlink" href="#principal-component-analysis" title="Link to this heading">¶</a></h1>
<section id="background">
<h2>Background<a class="headerlink" href="#background" title="Link to this heading">¶</a></h2>
<p>Principal component analysis (PCA), also known as Karhunen-Loeve
transform, is arguably the most fundamental technique in
unUnsupervised learning. It is frequently used for (linear)
dimensionality reduction, (lossy) data compression, feature
extraction, and data visualization.  Let us consider a set of <em>l</em> data
points</p>
<div class="math notranslate nohighlight">
\[S=\{x_1,\dots, x_l\} .\]</div>
<p>Let <em>n</em> real-valued attributes
(<em>n</em> features) represent a single data point. How can we reduce the
length of the description to <em>m &lt; n</em> variables such that as much
information as possible is preserved? How many dimensions are needed
to capture a certain percentage of the variability of the data? These
are typical questions that arise when we want to do dimensionality
reduction, feature selection, and compression (these three terms
usually refer to similar processes, emphasizing different aspects of
the same algorithmic procedure). Dimensionality reduction is closely
linked to visualization. When visualizing high-dimensional data, the
question arises of how to project the data to two or three dimensions
such that the visualization of these dimensions reflects as much of
the variability of the data as possible.  Principle component analysis
gives (under particular assumptions) answers to these questions.</p>
<p>The goal of PCA is to find an <em>m</em>-dimensional affine linear model of
the <em>n</em>-dimensional data points in <em>S</em> that represents the original
data as accurately as possible in a least-squares sense.  This model
does not only minimize the reconstruction error when mapping the data
back to the original space, it is also the affine linear model that
yields the representation that maximizes the overall variance when
encoding the points in <em>S</em> using only <em>m</em> dimensions. For details see
for instance <a class="reference internal" href="#dmln5" id="id1"><span>[DMLN5]</span></a>.</p>
</section>
<section id="pca-in-shark">
<h2>PCA in Shark<a class="headerlink" href="#pca-in-shark" title="Link to this heading">¶</a></h2>
<p>The following includes are needed for this tutorial:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;shark/Algorithms/Trainers/PCA.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;shark/Data/Pgm.h&gt;</span>
</pre></div>
</div>
<section id="face-recognition-problem">
<h3>Face recognition problem<a class="headerlink" href="#face-recognition-problem" title="Link to this heading">¶</a></h3>
<p>As a classical example for PCA, let us consider computing eigenfaces
<a class="reference internal" href="#turkpentland1991" id="id2"><span>[TurkPentland1991]</span></a> using the Cambridge Face Database
<a class="reference internal" href="#samariaharter1994" id="id3"><span>[SamariaHarter1994]</span></a> as an example. It contains 92x122 images of
frontal faces, some examples are shown in Figure 5.2. We can represent
each face by a vector by inflating the image, that is, by
concatenating the image rows. Although the images are rather small, we
get a 10304-dimensional representation (i.e., <em>n = 10304</em>). That is,
in the original representation 10304 basis vectors, one for each
pixel, define the image space. Principal component analysis can help
us to significantly reduce the description length of the images.</p>
<p>The data can be downloaded from <a class="reference external" href="http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html">here</a>.
Some sample faces are shown in the following figure.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../../../_images/faces.png"><img alt="plot of sample faces" src="../../../_images/faces.png" style="width: 432.0px; height: 324.0px;" />
</a>
</figure>
</section>
<section id="reading-in-the-data">
<h3>Reading in the data<a class="headerlink" href="#reading-in-the-data" title="Link to this heading">¶</a></h3>
<p>First, let us read in the data.  There is a function for recursively
scanning a directory for images in pgm format and reading them in:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">const</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">*</span><span class="n">facedirectory</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s">&quot;Cambridge_FaceDB&quot;</span><span class="p">;</span><span class="w"> </span><span class="c1">//&lt; set this to the directory containing the face database</span>
<span class="n">UnlabeledData</span><span class="o">&lt;</span><span class="n">RealVector</span><span class="o">&gt;</span><span class="w"> </span><span class="n">images</span><span class="p">;</span>

<span class="w">        </span><span class="n">importPGMSet</span><span class="p">(</span><span class="n">facedirectory</span><span class="p">,</span><span class="w"> </span><span class="n">images</span><span class="p">);</span>

<span class="kt">unsigned</span><span class="w"> </span><span class="n">l</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">images</span><span class="p">.</span><span class="n">numberOfElements</span><span class="p">();</span><span class="w">   </span><span class="c1">// number of samples</span>
<span class="kt">unsigned</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">images</span><span class="p">.</span><span class="n">shape</span><span class="p">()[</span><span class="mi">1</span><span class="p">];</span><span class="w"> </span><span class="c1">// width of images</span>
<span class="kt">unsigned</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">images</span><span class="p">.</span><span class="n">shape</span><span class="p">()[</span><span class="mi">0</span><span class="p">];</span><span class="w"> </span><span class="c1">// height of images</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">imagesInfo</span></code> struct contains sizes and names of the individual
images.</p>
</section>
<section id="models-and-learning-algorithm">
<h3>Models and learning algorithm<a class="headerlink" href="#models-and-learning-algorithm" title="Link to this heading">¶</a></h3>
<p>Doing a PCA is as simple as:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">PCA</span><span class="w"> </span><span class="nf">pca</span><span class="p">(</span><span class="n">images</span><span class="p">);</span>
</pre></div>
</div>
<p>Karhunen-Loeve transformations are affine linear models.
For encoding data to an <em>m</em>-dimensional subspace we use:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">unsigned</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">299</span><span class="p">;</span>
<span class="n">LinearModel</span><span class="o">&lt;&gt;</span><span class="w"> </span><span class="n">enc</span><span class="p">;</span>
<span class="n">pca</span><span class="p">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">enc</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">);</span>
</pre></div>
</div>
<p>The last line encodes (i.e., represents in the PCA coordinate system)
the whole image database.</p>
<p>We can easily map from the <em>m</em> dimensional space back to the original
<em>n</em> dimensional space by the optimal linear reconstruction (the
decoder):</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">LinearModel</span><span class="o">&lt;&gt;</span><span class="w"> </span><span class="n">dec</span><span class="p">;</span>
<span class="n">pca</span><span class="p">.</span><span class="n">decoder</span><span class="p">(</span><span class="n">dec</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">);</span>
</pre></div>
</div>
<p>For instance, let us reconstruct the following first image using just the
first <em>m=300</em> components.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../../../_images/face0.png"><img alt="first face in database" src="../../../_images/face0.png" style="width: 92.0px; height: 112.0px;" />
</a>
</figure>
<p>Then we write</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">unsigned</span><span class="w"> </span><span class="n">sampleImage</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>

<span class="n">boost</span><span class="o">::</span><span class="n">format</span><span class="w"> </span><span class="n">fmterRec</span><span class="p">(</span><span class="s">&quot;facesReconstruction%d-%d.pgm&quot;</span><span class="p">);</span>
<span class="n">exportPGM</span><span class="p">((</span><span class="n">fmterRec</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="n">sampleImage</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="n">m</span><span class="p">).</span><span class="n">str</span><span class="p">().</span><span class="n">c_str</span><span class="p">(),</span><span class="w"> </span><span class="n">dec</span><span class="p">(</span><span class="n">encodedImages</span><span class="p">.</span><span class="n">element</span><span class="p">(</span><span class="n">sampleImage</span><span class="p">)),</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">);</span>
</pre></div>
</div>
<p>and get the following image.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../../../_images/reconstruction0.png"><img alt="reconstruction of first face using 300 components" src="../../../_images/reconstruction0.png" style="width: 92.0px; height: 112.0px;" />
</a>
</figure>
</section>
<section id="further-evaluation-of-the-model">
<h3>Further evaluation of the model<a class="headerlink" href="#further-evaluation-of-the-model" title="Link to this heading">¶</a></h3>
<p>We can retrieve the eigenvalues and eigenvectors of the model by
calling <code class="docutils literal notranslate"><span class="pre">pca.eigenvalues()</span></code> and <code class="docutils literal notranslate"><span class="pre">pca.eigenvectors()</span></code>,
respectively.  The number of eigenvalues and eigenvectors returned by
these functions is min(<em>l</em>, <em>n</em>). The eigenvalue <em>i</em>  can also be
retrieved by <code class="docutils literal notranslate"><span class="pre">pca.eigenvalue(i)</span></code>.  Visualizing the mean face is done
by</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">exportPGM</span><span class="p">(</span><span class="s">&quot;facesMean.pgm&quot;</span><span class="p">,</span><span class="w"> </span><span class="n">pca</span><span class="p">.</span><span class="n">mean</span><span class="p">(),</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">);</span>
</pre></div>
</div>
<p>resulting in the following mean image.</p>
<figure class="align-default">
<a class="reference internal image-reference" href="../../../_images/facesMean.png"><img alt="reconstruction of first face using 200 components" src="../../../_images/facesMean.png" style="width: 92.0px; height: 112.0px;" />
</a>
</figure>
</section>
</section>
<section id="full-example-program">
<h2>Full example program<a class="headerlink" href="#full-example-program" title="Link to this heading">¶</a></h2>
<p>An extended eigenface example program is <a class="reference external" href="../../../../../../doxygen_pages/html/_p_c_a_tutorial_8cpp.html">PCATutorial.cpp</a>.
The face database can be downloaded from
<a class="reference external" href="http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html">here</a>.</p>
</section>
<section id="additional-features">
<h2>Additional features<a class="headerlink" href="#additional-features" title="Link to this heading">¶</a></h2>
<p>The Shark PCA automatically applies the “more attributes than data
points” trick, see <a class="reference internal" href="#dmln5" id="id5"><span>[DMLN5]</span></a>. It easily allows for “whitening”, that
is, learning a transformation giving unit variance of the sample data
in the new coordinate system along each component.</p>
<p>As always, please look at the file documentation, the example
programs, and the unit test (<code class="docutils literal notranslate"><span class="pre">Test/Algorithms/Trainers</span></code>
subdirectory).</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">¶</a></h2>
<div role="list" class="citation-list">
<div class="citation" id="dmln5" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>DMLN5<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id1">1</a>,<a role="doc-backlink" href="#id5">2</a>)</span>
<p>C. Igel.
Data Mining: Lecture Notes, chapter 5, 2011</p>
</div>
<div class="citation" id="samariaharter1994" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">SamariaHarter1994</a><span class="fn-bracket">]</span></span>
<p>F. Samaria and A. Harter.
Parameterisation of a stochastic model for human face
identification. In IEEE Workshop on Applications of Computer
Vision, pages 138-142. IEEE Computer Society Press, 1994.</p>
</div>
<div class="citation" id="turkpentland1991" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">TurkPentland1991</a><span class="fn-bracket">]</span></span>
<p>M. Turk and A. Pentland. Eigenfaces for
recognition. Journal of Cognitive Neuroscience, 3(1):71-86, 1991.</p>
</div>
</div>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
	<div class="mt_ltocwrapper">
		<ul>
<li><a class="reference internal" href="#">Principal Component Analysis</a><ul>
<li><a class="reference internal" href="#background">Background</a></li>
<li><a class="reference internal" href="#pca-in-shark">PCA in Shark</a></li>
<li><a class="reference internal" href="#full-example-program">Full example program</a></li>
<li><a class="reference internal" href="#additional-features">Additional features</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

	</div>
<div>
  <a class="topless" href="../concepts/misc/serialization.html" title="previous chapter">
	  <img class="navicon" src="../../../_static/icon_backward.png" alt="prev"/> Serialization</a>
  <a class="topless" href="lda.html" title="next chapter">
	  <img class="navicon" src="../../../_static/icon_forward.png" alt="next"/> Linear Discriminant Analysis</a>
</div> 
<div id="searchbox" style="display: none">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" size="12" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
      <input class="mtsubmitbutton" type="submit" value="Find" />
    </form>
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
<p class="mtshowsource">
  <a href="../../../_sources/rest_sources/tutorials/algorithms/pca.rst.txt"
           rel="nofollow"><img class="sourceicon" src="../../../_static/icon_eject.png" alt="prev"/> Show page source</a>
</p>
        </div>
      </div>
      <div class="clearer"></div>
    </div>

    <div class="footer">
        <div class="footerlogos">
            <a href="http://validator.w3.org/check/referer" title="Valid XHTML 1.0">
                <img class="footerlogos" src="../../../_static/xhtml_validation.png" alt="Valid XHTML 1.0" />
            </a>
            <a href="http://jigsaw.w3.org/css-validator/check/referer?profile=css3" title="Valid CSS3">
                <img class="footerlogos" src="../../../_static/css_validation.png" alt="Valid CSS3" />
            </a>
        </div>
            &copy; The Shark developer team.
           Created on 21/05/2024
           using <a href="http://sphinx.pocoo.org/">Sphinx</a> 7.3.7
    </div>
  </body>
</html>