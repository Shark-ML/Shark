<!DOCTYPE html>

<html lang="en" data-content_root="../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    
    <title>Training Feed-Forward Neural Networks &#8212; Shark 3.0a documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mt_sphinx_deriv.css?v=6ec4729f" />
    <script src="../../../_static/documentation_options.js?v=db277b1a"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML&delayStartupUntilConfig"></script>
    <link rel="icon" href="../../../_static/shark16.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Data Shapes and Deep Convolutional Architectures" href="DeepMNIST.html" />
    <link rel="prev" title="LASSO Regression" href="LASSO.html" />
    <link rel="stylesheet" href="../../../_static/mt_sphinx_shark.css" type="text/css" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js?config=TeX-AMS_CHTML"></script>
    <script src="../../../index/../../../../mlstyle.js"></script>

  </head><body>

    <div id="shark_old">
        <div id="wrap">
            <div id="header">
                <div id="site-name"><a href="/Shark/index.html">Shark machine learning library</a></div>
                <ul id="nav">
                    <li  class="first" >
                        <a href="../../installation.html">Installation</a>
                    </li>
                    <li  class="active" >
                        <a href="../tutorials.html">Tutorials</a>
                    </li>
		    <li  class="first" >
                        <a href="../../benchmark.html">Benchmarks</a>
                    </li>
		    <li  class="first" >
                        <a href="../../../index/../../../../doxygen_pages/html/classes.html">Documentation</a>
                        <ul>
                            <li><a href="../../quickref/quickref.html">Quick references</a></li>
                            <li><a href="../../../index/../../../../doxygen_pages/html/classes.html">Class list</a></li>
                            <li class="last"><a href="../../../index/../../../../doxygen_pages/html/group__shark__globals.html">Global functions</a></li>
                        </ul>
                    </li>
                </ul>

            </div>
        </div>
    </div>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="training-feed-forward-neural-networks">
<h1>Training Feed-Forward Neural Networks<a class="headerlink" href="#training-feed-forward-neural-networks" title="Link to this heading">¶</a></h1>
<p>This tutorial will show you how to construct a feed-forward
multi-layer neural network and how to train it efficiently using minibatch training
and the Adam Optimization algorithm. It is recommended to read the
getting started section, especially the introduction about <a class="reference internal" href="../first_steps/general_optimization_tasks.html"><span class="doc">General Optimization Tasks</span></a>.
The full example program can be found here: <a class="reference external" href="../../../../../../doxygen_pages/html/_f_f_n_n_basic_tutorial_8cpp.html">FFNNBasicTutorial.cpp</a>.</p>
<p>For this tutorial the following includes are needed:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">//the model</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;shark/Models/LinearModel.h&gt;</span><span class="c1">//single dense layer</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;shark/Models/ConcatenatedModel.h&gt;</span><span class="c1">//for stacking layers, provides operator&gt;&gt;</span>
<span class="c1">//training the  model</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;shark/ObjectiveFunctions/ErrorFunction.h&gt;</span><span class="c1">//error function, allows for minibatch training</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;shark/ObjectiveFunctions/Loss/CrossEntropy.h&gt;</span><span class="c1"> // loss used for supervised training</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;shark/ObjectiveFunctions/Loss/ZeroOneLoss.h&gt;</span><span class="c1"> // loss used for evaluation of performance</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;shark/Algorithms/GradientDescent/Adam.h&gt;</span><span class="c1"> //optimizer: simple gradient descent.</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;shark/Data/SparseData.h&gt;</span><span class="c1"> //loading the dataset</span>
<span class="k">using</span><span class="w"> </span><span class="k">namespace</span><span class="w"> </span><span class="nn">shark</span><span class="p">;</span>
</pre></div>
</div>
<section id="loading-the-data">
<h2>Loading the Data<a class="headerlink" href="#loading-the-data" title="Link to this heading">¶</a></h2>
<p>In this tutorial, we want to learn to recognize handwriten digits from the mnist dataset
using a simple feed-forward network. This is done similar to the previous tutorials
by loading a dataset and splitting it into training and test part:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="kt">size_t</span><span class="w"> </span><span class="n">batchSize</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">256</span><span class="p">;</span>
<span class="n">LabeledData</span><span class="o">&lt;</span><span class="n">RealVector</span><span class="p">,</span><span class="kt">unsigned</span><span class="w"> </span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">data</span><span class="p">;</span>
<span class="n">importSparseData</span><span class="p">(</span><span class="w"> </span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">batchSize</span><span class="w"> </span><span class="p">);</span>
<span class="n">data</span><span class="p">.</span><span class="n">shuffle</span><span class="p">();</span><span class="w"> </span><span class="c1">//shuffle data randomly</span>
<span class="k">auto</span><span class="w"> </span><span class="n">test</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">splitAtElement</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="mi">70</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">data</span><span class="p">.</span><span class="n">numberOfElements</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="mi">100</span><span class="p">);</span><span class="c1">//split a test set</span>
<span class="n">std</span><span class="o">::</span><span class="kt">size_t</span><span class="w"> </span><span class="n">numClasses</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">numberOfClasses</span><span class="p">(</span><span class="n">data</span><span class="p">);</span>
<span class="n">std</span><span class="o">::</span><span class="kt">size_t</span><span class="w"> </span><span class="n">inputDim</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputDimension</span><span class="p">(</span><span class="n">data</span><span class="p">);</span>
</pre></div>
</div>
<p>Note that we define the batchsize of the dataset during loading of the dataset.
The batchsize is important for minibatch learning as we pick one random batch from the dataset in each iteration.</p>
</section>
<section id="defining-the-network-topology">
<h2>Defining the Network topology<a class="headerlink" href="#defining-the-network-topology" title="Link to this heading">¶</a></h2>
<p>For this, we stack several layers on top of each other. For simplicity
we will use only <a class="reference external" href="../../../../../../doxygen_pages/html/classshark_1_1_linear_model.html">LinearModel</a> with nonlinear <a class="reference external" href="../../../../../../doxygen_pages/html/group__activations.html">activation functions</a>. The activation
function is encoded in an optional second template argument of the linear model.
For this tutorial we decide that the two hidden layers should use
Rectified-Linear Units (ReLu). For classification tasks, output neurons should be
linear. All layers should use a constant offset term. The layers are then chained together using <cite>operator&gt;&gt;</cite>:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">//We use a dense linear model with rectifier activations</span>
<span class="k">typedef</span><span class="w"> </span><span class="n">LinearModel</span><span class="o">&lt;</span><span class="n">RealVector</span><span class="p">,</span><span class="w"> </span><span class="n">RectifierNeuron</span><span class="o">&gt;</span><span class="w"> </span><span class="n">DenseLayer</span><span class="p">;</span>

<span class="c1">//build the network</span>
<span class="n">DenseLayer</span><span class="w"> </span><span class="nf">layer1</span><span class="p">(</span><span class="n">inputDim</span><span class="p">,</span><span class="n">hidden1</span><span class="p">,</span><span class="w"> </span><span class="nb">true</span><span class="p">);</span>
<span class="n">DenseLayer</span><span class="w"> </span><span class="nf">layer2</span><span class="p">(</span><span class="n">hidden1</span><span class="p">,</span><span class="n">hidden2</span><span class="p">,</span><span class="w"> </span><span class="nb">true</span><span class="p">);</span>
<span class="n">LinearModel</span><span class="o">&lt;</span><span class="n">RealVector</span><span class="o">&gt;</span><span class="w"> </span><span class="n">output</span><span class="p">(</span><span class="n">hidden2</span><span class="p">,</span><span class="n">numClasses</span><span class="p">,</span><span class="w"> </span><span class="nb">true</span><span class="p">);</span>
<span class="k">auto</span><span class="w"> </span><span class="n">network</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">layer1</span><span class="w"> </span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">layer2</span><span class="w"> </span><span class="o">&gt;&gt;</span><span class="w"> </span><span class="n">output</span><span class="p">;</span>
</pre></div>
</div>
<p>As you can see, we define the number of inputs and outputs and usage of the offset term for each layer during construction.</p>
</section>
<section id="training-the-network">
<h2>Training the Network<a class="headerlink" href="#training-the-network" title="Link to this heading">¶</a></h2>
<p>After we loaded the dataset and defined the topology of the network,
we can train it. Since we use a classification task, we can
use the <a class="reference external" href="../../../../../../doxygen_pages/html/classshark_1_1_cross_entropy.html">CrossEntropy</a> error to maximize the class probability.
The cross entropy assumes the inputs to be the log of
the unnormalized probability <span class="math notranslate nohighlight">\(p(y=c|x)\)</span>, i.e. the probability of
the input to belong to class <span class="math notranslate nohighlight">\(c\)</span>. The cross entropy uses an
exponential normalisation to transform the inputs into proper
normalised probabilities. This is done in a numericaly stable way.</p>
<p>The c-th output neuron of the network encodes the probability of class c.
In case of a binary problem, we can omit one
output neuron and in this case, it is assumed that the output of the
<cite>imaginary</cite> second neuron is just the negative of the first. The loss
function takes care of the normalisation. After training, the most
likely class label of an input can be evaluated by picking the class
of the neuron with highest activation value.  In the case of only one
output neuron, the sign decides: negative activation is class 0,
positive is class 1.</p>
<p>We will setup our error function to use minibatch training. Every time
the error function is evaluated, a random batch in the dataset is evaluated.
Thus the batch size defined during dataset creation is an important parameter and a trade-off betwen evaluation
speed and noise of the evaluation:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">//create the supervised problem.</span>
<span class="n">CrossEntropy</span><span class="o">&lt;</span><span class="kt">unsigned</span><span class="w"> </span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="n">RealVector</span><span class="o">&gt;</span><span class="w"> </span><span class="n">loss</span><span class="p">;</span>
<span class="n">ErrorFunction</span><span class="o">&lt;&gt;</span><span class="w"> </span><span class="n">error</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">network</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">loss</span><span class="p">,</span><span class="w"> </span><span class="nb">true</span><span class="p">);</span><span class="c1">//enable minibatch training</span>

<span class="c1">//optimize the model</span>
<span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="o">&lt;&lt;</span><span class="s">&quot;training network&quot;</span><span class="o">&lt;&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="n">initRandomNormal</span><span class="p">(</span><span class="n">network</span><span class="p">,</span><span class="mf">0.001</span><span class="p">);</span>
<span class="n">Adam</span><span class="o">&lt;&gt;</span><span class="w"> </span><span class="n">optimizer</span><span class="p">;</span>
<span class="n">error</span><span class="p">.</span><span class="n">init</span><span class="p">();</span>
<span class="n">optimizer</span><span class="p">.</span><span class="n">init</span><span class="p">(</span><span class="n">error</span><span class="p">);</span>
<span class="k">for</span><span class="p">(</span><span class="n">std</span><span class="o">::</span><span class="kt">size_t</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">iterations</span><span class="p">;</span><span class="w"> </span><span class="o">++</span><span class="n">i</span><span class="p">){</span>
<span class="w">        </span><span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">(</span><span class="n">error</span><span class="p">);</span>
<span class="w">        </span><span class="n">std</span><span class="o">::</span><span class="n">cout</span><span class="o">&lt;&lt;</span><span class="n">i</span><span class="o">&lt;&lt;</span><span class="s">&quot; &quot;</span><span class="o">&lt;&lt;</span><span class="n">optimizer</span><span class="p">.</span><span class="n">solution</span><span class="p">().</span><span class="n">value</span><span class="o">&lt;&lt;</span><span class="n">std</span><span class="o">::</span><span class="n">endl</span><span class="p">;</span>
<span class="p">}</span>
<span class="n">network</span><span class="p">.</span><span class="n">setParameterVector</span><span class="p">(</span><span class="n">optimizer</span><span class="p">.</span><span class="n">solution</span><span class="p">().</span><span class="n">point</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="what-you-learned">
<h2>What you learned<a class="headerlink" href="#what-you-learned" title="Link to this heading">¶</a></h2>
<p>You should have learned the following aspects in this Tutorial:</p>
<ul class="simple">
<li><p>How to stack models to create a feed-forward neural network</p></li>
<li><p>How to choose activation functions</p></li>
<li><p>How to perform batch learning using the <a class="reference external" href="../../../../../../doxygen_pages/html/classshark_1_1_error_function.html">ErrorFunction</a></p></li>
</ul>
</section>
<section id="what-next">
<h2>What next?<a class="headerlink" href="#what-next" title="Link to this heading">¶</a></h2>
<p>The network architecture used for training MNIST is not state of the art. In real applications, deep convolutional architectures are used.
This is covered in the next tutorial, <a class="reference internal" href="DeepMNIST.html"><span class="doc">Data Shapes and Deep Convolutional Architectures</span></a>.</p>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
	<div class="mt_ltocwrapper">
		<ul>
<li><a class="reference internal" href="#">Training Feed-Forward Neural Networks</a><ul>
<li><a class="reference internal" href="#loading-the-data">Loading the Data</a></li>
<li><a class="reference internal" href="#defining-the-network-topology">Defining the Network topology</a></li>
<li><a class="reference internal" href="#training-the-network">Training the Network</a></li>
<li><a class="reference internal" href="#what-you-learned">What you learned</a></li>
<li><a class="reference internal" href="#what-next">What next?</a></li>
</ul>
</li>
</ul>

	</div>
<div>
  <a class="topless" href="LASSO.html" title="previous chapter">
	  <img class="navicon" src="../../../_static/icon_backward.png" alt="prev"/> LASSO Regression</a>
  <a class="topless" href="DeepMNIST.html" title="next chapter">
	  <img class="navicon" src="../../../_static/icon_forward.png" alt="next"/> Data Shapes and Deep Convolutional Architectures</a>
</div> 
<div id="searchbox" style="display: none">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" size="12" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
      <input class="mtsubmitbutton" type="submit" value="Find" />
    </form>
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
<p class="mtshowsource">
  <a href="../../../_sources/rest_sources/tutorials/algorithms/ffnet.rst.txt"
           rel="nofollow"><img class="sourceicon" src="../../../_static/icon_eject.png" alt="prev"/> Show page source</a>
</p>
        </div>
      </div>
      <div class="clearer"></div>
    </div>

    <div class="footer">
        <div class="footerlogos">
            <a href="http://validator.w3.org/check/referer" title="Valid XHTML 1.0">
                <img class="footerlogos" src="../../../_static/xhtml_validation.png" alt="Valid XHTML 1.0" />
            </a>
            <a href="http://jigsaw.w3.org/css-validator/check/referer?profile=css3" title="Valid CSS3">
                <img class="footerlogos" src="../../../_static/css_validation.png" alt="Valid CSS3" />
            </a>
        </div>
            &copy; The Shark developer team.
           Created on 21/05/2024
           using <a href="http://sphinx.pocoo.org/">Sphinx</a> 7.3.7
    </div>
  </body>
</html>