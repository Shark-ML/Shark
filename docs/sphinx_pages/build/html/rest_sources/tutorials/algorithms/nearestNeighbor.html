<!DOCTYPE html>

<html lang="en" data-content_root="../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    
    <title>Nearest Neighbor Classification &#8212; Shark 3.0a documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mt_sphinx_deriv.css?v=6ec4729f" />
    <script src="../../../_static/documentation_options.js?v=db277b1a"></script>
    <script src="../../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML&delayStartupUntilConfig"></script>
    <link rel="icon" href="../../../_static/shark16.ico"/>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="K-Means Clustering" href="kmeans.html" />
    <link rel="prev" title="Linear Regression" href="linearRegression.html" />
    <link rel="stylesheet" href="../../../_static/mt_sphinx_shark.css" type="text/css" />
    <link rel="stylesheet" href="../../../index.html" type="text/css" />
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.1/es5/tex-mml-chtml.js"></script>
    <script src="../../../index/../../../../mlstyle.js"></script>

  </head><body>

    <div id="shark_old">
        <div id="wrap">
            <div id="header">
                <div id="site-name"><a href="/Shark/index.html">Shark machine learning library</a></div>
                <ul id="nav">
                    <li  class="first" >
                        <a href="../../installation.html">Installation</a>
                    </li>
                    <li  class="active" >
                        <a href="../tutorials.html">Tutorials</a>
                    </li>
		    <li  class="first" >
                        <a href="../../benchmark.html">Benchmarks</a>
                    </li>
		    <li  class="first" >
                        <a href="../../../index/../../../../doxygen_pages/html/classes.html">Documentation</a>
                        <ul>
                            <li><a href="../../quickref/quickref.html">Quick references</a></li>
                            <li><a href="../../../index/../../../../doxygen_pages/html/classes.html">Class list</a></li>
                            <li class="last"><a href="../../../index/../../../../doxygen_pages/html/group__shark__globals.html">Global functions</a></li>
                        </ul>
                    </li>
                </ul>

            </div>
        </div>
    </div>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="nearest-neighbor-classification">
<h1>Nearest Neighbor Classification<a class="headerlink" href="#nearest-neighbor-classification" title="Link to this heading">¶</a></h1>
<section id="background">
<h2>Background<a class="headerlink" href="#background" title="Link to this heading">¶</a></h2>
<p>The nearest neighbor classifier is a simple, still powerful classification algorithm.
Assume that we are given a data set</p>
<div class="math notranslate nohighlight">
\[S = \{(x_1,y_1),...,(x_l,y_l)\}\]</div>
<p>and that we want to predict the label <em>y</em> of an unseen data point <em>x</em>.
The idea of <em>K</em> nearest neighbor classification is to look in <em>S</em> for
those <em>K</em> patterns that are most similar to <em>x</em> and to choose <em>y</em>
based on their labels. The NearestNeighborModel implemented in Shark
supports classification as well as regression. In this tutorial we give an classification
example</p>
<p>For details see <a class="reference internal" href="#dmlnb" id="id1"><span>[DMLNb]</span></a>.</p>
</section>
<section id="nearest-neighbor-classification-in-shark">
<h2>Nearest Neighbor Classification in Shark<a class="headerlink" href="#nearest-neighbor-classification-in-shark" title="Link to this heading">¶</a></h2>
<section id="sample-classification-problem-protein-fold-prediction">
<h3>Sample classification problem: Protein fold prediction<a class="headerlink" href="#sample-classification-problem-protein-fold-prediction" title="Link to this heading">¶</a></h3>
<p>Proteins are important building blocks of our body and it is essential
to understand their biological function not only for purely scientific
reasons but also for drug discovery and developing of treatments
schemes against diseases.</p>
<p>It is known that the function of a protein is closely related to its
3D spatial structure. Biologists developed several experimental
methods to determine the 3D structure of a protein including protein
nuclear magnetic resonance (NMR) or X-ray based techniques. However,
these experimental techniques are generally time consuming, slow, and
very expensive.</p>
<p>Against this background, predicting the spatial structure based on the
gene sequence and other information about a protein is an important
task in bioinformatics. Here we consider prediction of the secondary
structure of proteins, that is, their general shape ignoring specific
atomic positions in three-dimensional space. The goal is to assign a
protein to one out of 27 SCOP fold types <a class="reference internal" href="#dingdubchak2001" id="id2"><span>[DingDubchak2001]</span></a>.</p>
<p>We consider classification using descriptions of amino-acid sequences
based on the data provided by <a class="reference internal" href="#damoulasgirolami2008" id="id3"><span>[DamoulasGirolami2008]</span></a>.  The
data <a class="reference download internal" download="" href="../../../_downloads/24032269c8137371b4d3238c3e8b77d2/C.csv"><code class="xref download docutils literal notranslate"><span class="pre">C.csv</span></code></a> provide a description of the
amino-acid compositions of 695 proteins together with the
corresponding fold type. Each row corresponds to a protein. The first
20 attributes in each line of <a class="reference download internal" download="" href="../../../_downloads/24032269c8137371b4d3238c3e8b77d2/C.csv"><code class="xref download docutils literal notranslate"><span class="pre">C.csv</span></code></a> are the
features (input attributes), the final integer indicates the class
(i.e., the fold type).</p>
</section>
<section id="reading-inspecting-and-splitting-the-data">
<h3>Reading, inspecting and splitting the data<a class="headerlink" href="#reading-inspecting-and-splitting-the-data" title="Link to this heading">¶</a></h3>
<p>First, let us read in the data
from the file <a class="reference download internal" download="" href="../../../_downloads/24032269c8137371b4d3238c3e8b77d2/C.csv"><code class="xref download docutils literal notranslate"><span class="pre">C.csv</span></code></a>.
assuming</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;shark/Data/Csv.h&gt;</span>

<span class="k">using</span><span class="w"> </span><span class="k">namespace</span><span class="w"> </span><span class="nn">shark</span><span class="p">;</span>
<span class="k">using</span><span class="w"> </span><span class="k">namespace</span><span class="w"> </span><span class="nn">std</span><span class="p">;</span>
</pre></div>
</div>
<p>Then code for reading the data may look like this:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="kt">int</span><span class="w"> </span><span class="nf">main</span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">argc</span><span class="p">,</span><span class="w"> </span><span class="kt">char</span><span class="w"> </span><span class="o">**</span><span class="n">argv</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="k">if</span><span class="p">(</span><span class="n">argc</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mi">2</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;usage: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">argv</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; (filename)&quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="p">;</span>
<span class="w">                </span><span class="n">exit</span><span class="p">(</span><span class="n">EXIT_FAILURE</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">        </span><span class="c1">// read data</span>
<span class="w">        </span><span class="n">ClassificationDataset</span><span class="w"> </span><span class="n">data</span><span class="p">;</span>
<span class="w">        </span><span class="k">try</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">importCSV</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="w"> </span><span class="n">LAST_COLUMN</span><span class="p">,</span><span class="w"> </span><span class="sc">&#39; &#39;</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">        </span><span class="k">catch</span><span class="w"> </span><span class="p">(...)</span><span class="w"> </span><span class="p">{</span>
<span class="w">                </span><span class="n">cerr</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;unable to read data from file &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w">  </span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="p">;</span>
<span class="w">                </span><span class="n">exit</span><span class="p">(</span><span class="n">EXIT_FAILURE</span><span class="p">);</span>
<span class="w">        </span><span class="p">}</span>
</pre></div>
</div>
<p>The function <code class="docutils literal notranslate"><span class="pre">importCSV</span></code> is used for loading the data from the
files, as described in the <a class="reference internal" href="../concepts/data/import_data.html"><span class="doc">Importing Data</span></a> tutorial.  In this
example, the inputs and the labels are combined in a single file. The
argument <code class="docutils literal notranslate"><span class="pre">LAST_COLUMN</span></code> specifies that the label is the last entry in
a line. The argument <code class="docutils literal notranslate"><span class="pre">&quot;</span> <span class="pre">&quot;</span></code> specifies the character separating fields
in each line and <code class="docutils literal notranslate"><span class="pre">&quot;#&quot;</span></code> defines the character that marks comments in
the data file (i.e., lines starting with # are ignored).</p>
<p>Let’s inspect the data:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;number of data points: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">data</span><span class="p">.</span><span class="n">numberOfElements</span><span class="p">()</span>
<span class="w">     </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; number of classes: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">numberOfClasses</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="w">     </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot; input dimension: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">inputDimension</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
<p>Now we know that we have 694 data points points. Every protein is
described by 20 features/attributes and is assigned to one out of 20 classes.</p>
<p>Next, we split our data into a training and a test set used for
identifying and evaluating the model, respectively.
The command</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">ClassificationDataset</span><span class="w"> </span><span class="n">dataTest</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">splitAtElement</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="w"> </span><span class="k">static_cast</span><span class="o">&lt;</span><span class="n">std</span><span class="o">::</span><span class="kt">size_t</span><span class="o">&gt;</span><span class="p">(</span><span class="mf">.5</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">data</span><span class="p">.</span><span class="n">numberOfElements</span><span class="p">()));</span>
<span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;training data points: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">data</span><span class="p">.</span><span class="n">numberOfElements</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="p">;</span>
<span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;test data points: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">dataTest</span><span class="p">.</span><span class="n">numberOfElements</span><span class="p">()</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
<p>splits the data after the 311th element into two parts. the left part stays in the dataset,
the right part will now be our test set.</p>
</section>
<section id="model-and-learning-algorithm">
<h3>Model and learning algorithm<a class="headerlink" href="#model-and-learning-algorithm" title="Link to this heading">¶</a></h3>
<p>Efficient look-up of the stored training patterns is crucial if the
nearest neighbor method is applied to large data sets.  Tree lookup algorithms
are an efficient means for this, however they only work in low dimensional spaces,
or high dimensional spaces with low intrinsic dimensionality. For high dimensional
data, tree lookup is inefficient and actually may be slower than just evaluating the
distance between every pair of points. Thus we provide an implementation of nearest
neighor classifiers which allow to choose which algorithm to use.</p>
<p>The header files for the different algorithms required in this tutorial are:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;shark/Models/NearestNeighborModel.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;shark/Algorithms/NearestNeighbors/TreeNearestNeighbors.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;shark/Models/Trees/KDTree.h&gt;</span>
</pre></div>
</div>
<p>Since our dataset has low dimensionality, we can use a tree lookup.
The standard choice for a tree is a KD-tree <a class="reference internal" href="#bentley1975" id="id4"><span>[Bentley1975]</span></a>,
which is initialized with the training input data as follows:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">KDTree</span><span class="o">&lt;</span><span class="n">RealVector</span><span class="o">&gt;</span><span class="w"> </span><span class="n">tree</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">inputs</span><span class="p">());</span>
</pre></div>
</div>
<p>Now we generate the KNN-Lookup algorithm for this tree:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">TreeNearestNeighbors</span><span class="o">&lt;</span><span class="n">RealVector</span><span class="p">,</span><span class="kt">unsigned</span><span class="w"> </span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">algorithm</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="o">&amp;</span><span class="n">tree</span><span class="p">);</span>
</pre></div>
</div>
<p>Now defining and training a <em>K</em> nearest neighbor classifier with the algorithm is just a single line</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="k">const</span><span class="w"> </span><span class="kt">unsigned</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">K</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span><span class="w"> </span><span class="c1">// number of neighbors for kNN</span>
<span class="n">NearestNeighborModel</span><span class="o">&lt;</span><span class="n">RealVector</span><span class="p">,</span><span class="w"> </span><span class="kt">unsigned</span><span class="w"> </span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">KNN</span><span class="p">(</span><span class="o">&amp;</span><span class="n">algorithm</span><span class="p">,</span><span class="n">K</span><span class="p">);</span>
</pre></div>
</div>
<p>instantiating a  <a class="reference external" href="../../../../../../doxygen_pages/html/classshark_1_1_nearest_neighbor_model.html">NearestNeighborModel</a> object.</p>
</section>
<section id="evaluating-the-model">
<h3>Evaluating the model<a class="headerlink" href="#evaluating-the-model" title="Link to this heading">¶</a></h3>
<p>After training the model, we can evaluate it.  As a performance
measure, we consider the standard 0-1 loss.  The corresponding risk is
the probability of error, the empirical risk is the average
classification error.  When measured on set of sample patterns, it
simply computes the fraction of wrong predictions.
We define the <a class="reference external" href="../../../../../../doxygen_pages/html/classshark_1_1_zero_one_loss.html">ZeroOneLoss</a> for <code class="docutils literal notranslate"><span class="pre">unsigned</span> <span class="pre">integer</span></code> labels and
apply the classifier to the training and the test data:</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="n">ZeroOneLoss</span><span class="o">&lt;</span><span class="kt">unsigned</span><span class="w"> </span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">loss</span><span class="p">;</span>
<span class="n">Data</span><span class="o">&lt;</span><span class="kt">unsigned</span><span class="w"> </span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">prediction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">KNN</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">inputs</span><span class="p">());</span>
<span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">K</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;-KNN on training set accuracy: &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="mf">1.</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">loss</span><span class="p">.</span><span class="n">eval</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="n">labels</span><span class="p">(),</span><span class="w"> </span><span class="n">prediction</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="p">;</span>
<span class="n">prediction</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">KNN</span><span class="p">(</span><span class="n">dataTest</span><span class="p">.</span><span class="n">inputs</span><span class="p">());</span>
<span class="n">cout</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">K</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="s">&quot;-KNN on test set accuracy:     &quot;</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="mf">1.</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">loss</span><span class="p">.</span><span class="n">eval</span><span class="p">(</span><span class="n">dataTest</span><span class="p">.</span><span class="n">labels</span><span class="p">(),</span><span class="w"> </span><span class="n">prediction</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="n">endl</span><span class="p">;</span>
</pre></div>
</div>
<p>Of course, the accuracy is given by one minus the error.
The training accuracy for <em>K=1</em> is trivial, but it is interesting to
see how this simple classifier performs compared to random guessing.</p>
</section>
</section>
<section id="full-example-program">
<h2>Full example program<a class="headerlink" href="#full-example-program" title="Link to this heading">¶</a></h2>
<p>The full example program is <a class="reference external" href="../../../../../../doxygen_pages/html/_k_n_n_tutorial_8cpp.html">KNNTutorial.cpp</a>.</p>
</section>
<section id="advanced-topics">
<h2>Advanced topics<a class="headerlink" href="#advanced-topics" title="Link to this heading">¶</a></h2>
<p>Not only the Euclidean metric is supported, but also kernel-based
nearest neighbor classification is provided. In this case the
kernel-induced metric is applied. This can be done by replacing
the <a class="reference external" href="../../../../../../doxygen_pages/html/classshark_1_1_k_d_tree.html">KDTree</a> by an <a class="reference external" href="../../../../../../doxygen_pages/html/classshark_1_1_k_h_c_tree.html">KHCTree</a> and using an appropriate kernel.</p>
<p>For high dimensional data, as said before, a tree lookup may not be that efficient. In this case,
we can just use the simple brute force algorithm instead, which is
implemented by <a class="reference external" href="../../../../../../doxygen_pages/html/classshark_1_1_simple_nearest_neighbors.html">SimpleNearestNeighbors</a>. It requires specifying a kernel object.
Choosing a <a class="reference external" href="../../../../../../doxygen_pages/html/classshark_1_1_linear_kernel.html">LinearKernel</a> will give the same results as the example above.</p>
<p>Often you do not only want the nearest neighbor algorithm to predict
the most promising class label for given input pattern <em>x</em>, but an
estimate of the probability that <em>x</em> belongs to a certain class. This
can be done by using the decisionFunction() method provided by
the NearestNeighborModel in the classification case.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">¶</a></h2>
<div role="list" class="citation-list">
<div class="citation" id="bentley1975" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">Bentley1975</a><span class="fn-bracket">]</span></span>
<p>J.L. Bentley. Multidimensional binary search trees
used for associative searching. Communications of the ACM,
18(9):509-517, 1975.</p>
</div>
<div class="citation" id="damoulasgirolami2008" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">DamoulasGirolami2008</a><span class="fn-bracket">]</span></span>
<p>T. Damoulas and M. Girolami.
Probabilistic multi-class multi-kernel learning: on protein fold
recognition and remote homology detection. Bioinformatics,
24(10):1264-1270, 2008.</p>
</div>
<div class="citation" id="dingdubchak2001" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">DingDubchak2001</a><span class="fn-bracket">]</span></span>
<p>C.H.Q. Ding and I. Dubchak.  Multi-class
protein fold recognition using support vector machines and neural
networks. Bioinformatics, 17(4):349-358, 2001.</p>
</div>
<div class="citation" id="dmlnb" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">DMLNb</a><span class="fn-bracket">]</span></span>
<p>C. Igel.
Data Mining: Lecture Notes, chapter 2, 2011</p>
</div>
</div>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
	<div class="mt_ltocwrapper">
		<ul>
<li><a class="reference internal" href="#">Nearest Neighbor Classification</a><ul>
<li><a class="reference internal" href="#background">Background</a></li>
<li><a class="reference internal" href="#nearest-neighbor-classification-in-shark">Nearest Neighbor Classification in Shark</a></li>
<li><a class="reference internal" href="#full-example-program">Full example program</a></li>
<li><a class="reference internal" href="#advanced-topics">Advanced topics</a></li>
<li><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>

	</div>
<div>
  <a class="topless" href="linearRegression.html" title="previous chapter">
	  <img class="navicon" src="../../../_static/icon_backward.png" alt="prev"/> Linear Regression</a>
  <a class="topless" href="kmeans.html" title="next chapter">
	  <img class="navicon" src="../../../_static/icon_forward.png" alt="next"/> K-Means Clustering</a>
</div> 
<div id="searchbox" style="display: none">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" size="12" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
      <input class="mtsubmitbutton" type="submit" value="Find" />
    </form>
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
<p class="mtshowsource">
  <a href="../../../_sources/rest_sources/tutorials/algorithms/nearestNeighbor.rst.txt"
           rel="nofollow"><img class="sourceicon" src="../../../_static/icon_eject.png" alt="prev"/> Show page source</a>
</p>
        </div>
      </div>
      <div class="clearer"></div>
    </div>

    <div class="footer">
        <div class="footerlogos">
            <a href="http://validator.w3.org/check/referer" title="Valid XHTML 1.0">
                <img class="footerlogos" src="../../../_static/xhtml_validation.png" alt="Valid XHTML 1.0" />
            </a>
            <a href="http://jigsaw.w3.org/css-validator/check/referer?profile=css3" title="Valid CSS3">
                <img class="footerlogos" src="../../../_static/css_validation.png" alt="Valid CSS3" />
            </a>
        </div>
            &copy; The Shark developer team.
           Created on 21/05/2024
           using <a href="http://sphinx.pocoo.org/">Sphinx</a> 7.3.7
    </div>
  </body>
</html>